import torch
from torch.autograd  import Variable
import numpy as np
import torch.nn.functional as F
import matplotlib.pyplot as plt

x = torch.unsqueeze(torch.linspace(-1,1,100),dim=1)
y = x.pow(2)+0.2*torch.rand(x.size())

x,y = Variable(x),Variable(y)
## 数据显示
# plt.scatter(x.data.numpy(),y.data.numpy())
# plt.show()


class Net(torch.nn.Module):
    def __init__(self,n_features,n_hidden,n_output):
        super(Net,self).__init__() #继承net
        #隐藏层 ：输入->隐藏
        self.hidden = torch.nn.Linear(n_features,n_hidden)
        #预测： 隐藏层->结果输出
        self.predict = torch.nn.Linear(n_hidden,n_output)

    def forward(self,x):
        #搭建神经网络
        x = F.relu(self.hidden(x)) #利用激励函数激活
        x = self.predict(x)
        return x

if __name__=='__main__':
    net = Net(1,10,1)
    print(net)
    plt.ion()
    plt.show()
    #利用优化器优化神经网络
    optimizer = torch.optim.SGD(net.parameters(),lr=0.5)
    loss_func = torch.nn.MSELoss()

    for t in range(100):
        prediction = net(x)

        loss = loss_func(prediction,y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if t%5==0:
            plt.cla()
            plt.scatter(x.data.numpy(),y.data.numpy())
            plt.plot(x.data.numpy(),prediction.data.numpy(),'r-',lw=5)
            plt.text(0.5,0,'Loss=%.4f'%loss.data,fontdict={'size':20,'color':'red'})
            plt.pause(0.1)

    plt.ioff()
    plt.show()
